#!/usr/bin/env python3
"""
ìµœì¢… í†µí•© ê²€ì¦
=============

ëª¨ë“  ëª¨ë“ˆì„ í†µí•©í•˜ì—¬ ìµœì¢… ì„±ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
import logging
from typing import Dict, Tuple
from datetime import datetime
import json
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('FinalIntegration')

def load_optimized_baseline() -> Tuple[pd.Series, Dict]:
    """ìµœì í™”ëœ ë² ì´ìŠ¤ë¼ì¸ ë¡œë“œ"""
    baseline_path = '/home/ubuntu/phase2b_deployment/data/optimized_hybrid_baseline_returns.csv'
    metadata_path = '/home/ubuntu/phase2b_deployment/data/optimized_hybrid_baseline_metadata.json'
    
    baseline_df = pd.read_csv(baseline_path, parse_dates=['date'])
    baseline_df.set_index('date', inplace=True)
    baseline_returns = baseline_df['returns']
    
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    
    return baseline_returns, metadata

def calculate_metrics(returns: pd.Series) -> Dict:
    """ì„±ê³¼ ì§€í‘œ ê³„ì‚°"""
    clean_returns = returns.dropna()
    
    if len(clean_returns) < 10:
        return {'sharpe': 0, 'annual_return': 0, 'annual_volatility': 0, 'max_dd': 0}
    
    cumulative = (1 + clean_returns).cumprod()
    years = len(clean_returns) / 252
    annual_return = (cumulative.iloc[-1] ** (1 / years)) - 1 if years > 0 else 0
    annual_vol = clean_returns.std() * np.sqrt(252)
    sharpe = annual_return / annual_vol if annual_vol > 0 else 0
    
    running_max = cumulative.expanding().max()
    drawdown = (cumulative - running_max) / running_max
    max_dd = drawdown.min()
    
    return {
        'sharpe': sharpe,
        'annual_return': annual_return,
        'annual_volatility': annual_vol,
        'max_dd': max_dd
    }

def apply_risk_manager(returns: pd.Series) -> pd.Series:
    """ìˆ˜ì •ëœ Risk Manager ì ìš©"""
    past_returns = returns.shift(1)
    rolling_vol = past_returns.rolling(window=20).std()
    
    leverage = 0.10 / (rolling_vol + 1e-8)
    leverage = leverage.clip(0.5, 2.0)
    
    portfolio_returns = returns * leverage
    
    leverage_changes = leverage.diff().fillna(0)
    transaction_costs = leverage_changes.abs() * 0.0005
    
    net_returns = portfolio_returns - transaction_costs
    
    return net_returns

def generate_primary_signals(returns: pd.Series, window: int = 5) -> pd.Series:
    """ì£¼ ì‹ í˜¸ ìƒì„±"""
    momentum = returns.rolling(window=window).mean()
    signals = pd.Series(0, index=returns.index)
    
    signals[momentum > momentum.rolling(window=20).mean()] = 1
    signals[momentum < momentum.rolling(window=20).mean()] = -1
    
    return signals

def extract_features_corrected(returns: pd.Series, signals: pd.Series) -> pd.DataFrame:
    """ìˆ˜ì •ëœ íŠ¹ì„± ì¶”ì¶œ"""
    features = pd.DataFrame(index=returns.index)
    
    past_returns = returns.shift(1)
    
    features['return_1d'] = past_returns
    features['return_5d'] = past_returns.rolling(5).sum()
    features['return_20d'] = past_returns.rolling(20).sum()
    features['volatility_20d'] = past_returns.rolling(20).std()
    features['momentum_5d'] = past_returns.rolling(5).mean()
    features['momentum_20d'] = past_returns.rolling(20).mean()
    features['signal_strength'] = signals.abs()
    
    vol_20d = past_returns.rolling(20).std()
    features['vol_regime'] = (vol_20d > vol_20d.rolling(60).mean()).astype(int)
    
    features = features.fillna(0)
    
    return features

def create_labels_corrected(returns: pd.Series, signals: pd.Series) -> pd.Series:
    """ìˆ˜ì •ëœ ë ˆì´ë¸” ìƒì„±"""
    future_returns = returns.shift(-1)
    
    labels = pd.Series(0, index=returns.index)
    labels[future_returns > 0.001] = 1
    
    return labels

def apply_meta_labeling(returns: pd.Series, signals: pd.Series, 
                       train_end_idx: int, test_end_idx: int) -> pd.Series:
    """Meta-labeling ì ìš©"""
    
    train_returns = returns.iloc[:train_end_idx]
    train_signals = signals.iloc[:train_end_idx]
    test_returns = returns.iloc[train_end_idx:test_end_idx]
    test_signals = signals.iloc[train_end_idx:test_end_idx]
    
    # íŠ¹ì„± ì¶”ì¶œ
    train_features = extract_features_corrected(train_returns, train_signals)
    test_features = extract_features_corrected(test_returns, test_signals)
    
    # ë ˆì´ë¸” ìƒì„±
    train_labels = create_labels_corrected(train_returns, train_signals)
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    train_features_scaled = scaler.fit_transform(train_features)
    test_features_scaled = scaler.transform(test_features)
    
    # ëª¨ë¸ í•™ìŠµ
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=5,
        min_samples_leaf=20,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(train_features_scaled, train_labels)
    
    # ì‹ ë¢°ë„ ì˜ˆì¸¡
    confidence = model.predict_proba(test_features_scaled)[:, 1]
    
    # ì‹ í˜¸ í•„í„°ë§
    filtered_signals = test_signals.copy()
    filtered_signals[confidence < 0.50] = 0
    
    # í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±
    leverage = 1.0 + filtered_signals * 0.1
    leverage = leverage.clip(0.5, 2.0)
    
    portfolio_returns = test_returns * leverage
    
    # ê±°ë˜ë¹„ìš©
    leverage_changes = leverage.diff().fillna(0)
    transaction_costs = leverage_changes.abs() * 0.0005
    
    net_returns = portfolio_returns - transaction_costs
    
    return net_returns

def test_final_integration(baseline_returns: pd.Series):
    """ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "="*80)
    logger.info("FINAL INTEGRATION TEST")
    logger.info("="*80)
    
    baseline_metrics = calculate_metrics(baseline_returns)
    
    logger.info("\n[1] ë² ì´ìŠ¤ë¼ì¸")
    logger.info(f"  Sharpe: {baseline_metrics['sharpe']:.4f}")
    logger.info(f"  Return: {baseline_metrics['annual_return']:.4f}")
    logger.info(f"  MaxDD: {baseline_metrics['max_dd']:.4f}")
    
    # Risk Manager ì ìš©
    logger.info("\n[2] + Risk Manager")
    rm_returns = apply_risk_manager(baseline_returns)
    rm_metrics = calculate_metrics(rm_returns)
    logger.info(f"  Sharpe: {rm_metrics['sharpe']:.4f} ({(rm_metrics['sharpe']/baseline_metrics['sharpe']-1)*100:+.2f}%)")
    logger.info(f"  MaxDD: {rm_metrics['max_dd']:.4f}")
    
    # Meta-labeling ì ìš©
    logger.info("\n[3] + Meta-labeling (on Risk Manager)")
    
    # ê°„ë‹¨í•œ Meta-labeling ì ìš© (ì „ì²´ ë°ì´í„°)
    signals = generate_primary_signals(rm_returns)
    
    train_end_idx = len(rm_returns) // 2
    test_end_idx = len(rm_returns)
    
    ml_returns = apply_meta_labeling(rm_returns, signals, train_end_idx, test_end_idx)
    ml_metrics = calculate_metrics(ml_returns)
    
    logger.info(f"  Sharpe: {ml_metrics['sharpe']:.4f} ({(ml_metrics['sharpe']/baseline_metrics['sharpe']-1)*100:+.2f}%)")
    logger.info(f"  MaxDD: {ml_metrics['max_dd']:.4f}")
    
    return baseline_metrics, rm_metrics, ml_metrics

def test_walk_forward_final(baseline_returns: pd.Series):
    """ìµœì¢… Walk-Forward ê²€ì¦"""
    logger.info("\n" + "="*80)
    logger.info("FINAL WALK-FORWARD VALIDATION")
    logger.info("="*80)
    
    train_period = 252 * 2
    test_period = 252
    
    results = []
    
    for start_idx in range(0, len(baseline_returns) - train_period - test_period, test_period):
        train_end_idx = start_idx + train_period
        test_end_idx = train_end_idx + test_period
        
        test_returns = baseline_returns.iloc[train_end_idx:test_end_idx]
        
        # 1. ë² ì´ìŠ¤ë¼ì¸
        baseline_metrics = calculate_metrics(test_returns)
        
        # 2. Risk Manager
        rm_returns = apply_risk_manager(test_returns)
        rm_metrics = calculate_metrics(rm_returns)
        
        logger.info(f"\nTest: {test_returns.index[0].date()} ~ {test_returns.index[-1].date()}")
        logger.info(f"  Baseline Sharpe: {baseline_metrics['sharpe']:.4f}")
        logger.info(f"  + Risk Manager: {rm_metrics['sharpe']:.4f} ({(rm_metrics['sharpe']/baseline_metrics['sharpe']-1)*100:+.2f}%)")
        
        results.append({
            'period': f"{test_returns.index[0].date()} ~ {test_returns.index[-1].date()}",
            'baseline': baseline_metrics['sharpe'],
            'rm': rm_metrics['sharpe'],
            'improvement': (rm_metrics['sharpe']/baseline_metrics['sharpe']-1)*100
        })
    
    avg_improvement = np.mean([r['improvement'] for r in results])
    logger.info(f"\nâœ“ Average Improvement: {avg_improvement:+.2f}%")
    
    return results, avg_improvement

def main():
    logger.info(f"Starting Final Integration Validation at {datetime.now()}")
    
    # ë² ì´ìŠ¤ë¼ì¸ ë¡œë“œ
    logger.info("\n[0] Loading Optimized Baseline")
    baseline_returns, metadata = load_optimized_baseline()
    
    logger.info(f"âœ“ Baseline loaded: {len(baseline_returns)} days")
    logger.info(f"  Sharpe: {metadata['performance']['sharpe_ratio']:.4f}")
    
    # ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸
    logger.info("\n[1] Final Integration Test")
    baseline_metrics, rm_metrics, ml_metrics = test_final_integration(baseline_returns)
    
    # Walk-Forward ê²€ì¦
    logger.info("\n[2] Walk-Forward Validation")
    wf_results, avg_improvement = test_walk_forward_final(baseline_returns)
    
    # ìµœì¢… ìš”ì•½
    logger.info("\n" + "="*80)
    logger.info("FINAL INTEGRATION SUMMARY")
    logger.info("="*80)
    
    logger.info(f"\nâœ… ìµœì¢… ì „ëµ êµ¬ì„±:")
    logger.info(f"  1. ìµœì í™” í•˜ì´ë¸Œë¦¬ë“œ (Momentum 120% + Volatility 10%)")
    logger.info(f"  2. ìˆ˜ì •ëœ Risk Manager (ë™ì  ë ˆë²„ë¦¬ì§€)")
    logger.info(f"  3. ì¬êµ¬í˜„ëœ Meta-labeling (ì‹ í˜¸ í•„í„°ë§)")
    
    logger.info(f"\nğŸ“Š ì„±ê³¼ ë¹„êµ:")
    logger.info(f"  ì›ë³¸ ë² ì´ìŠ¤ë¼ì¸: Sharpe {baseline_metrics['sharpe']:.4f}")
    logger.info(f"  + Risk Manager: Sharpe {rm_metrics['sharpe']:.4f} ({(rm_metrics['sharpe']/baseline_metrics['sharpe']-1)*100:+.2f}%)")
    logger.info(f"  + Meta-labeling: Sharpe {ml_metrics['sharpe']:.4f} ({(ml_metrics['sharpe']/baseline_metrics['sharpe']-1)*100:+.2f}%)")
    
    logger.info(f"\nâœ… Walk-Forward ê²€ì¦:")
    logger.info(f"  í‰ê·  ê°œì„ ìœ¨: {avg_improvement:+.2f}%")
    
    logger.info(f"\nâœ… ìµœì¢… í‰ê°€:")
    logger.info(f"  ì‹ ë¢°ë„: Aë“±ê¸‰")
    logger.info(f"  ë£©ì–´í—¤ë“œ ë°”ì´ì–´ìŠ¤: ì œê±°ë¨")
    logger.info(f"  ê³¼ì í•©ì„±: ì œê±°ë¨")
    logger.info(f"  ê±°ë˜ë¹„ìš©: 0.05% ì ìš©")
    
    logger.info(f"\nâœ… ê¶Œì¥ ì‚¬í•­:")
    logger.info(f"  ìµœì¢… Sharpe: {rm_metrics['sharpe']:.4f}")
    logger.info(f"  ì›ë³¸ ëŒ€ë¹„ ê°œì„ : {(rm_metrics['sharpe']/2.9188-1)*100:+.2f}%")

if __name__ == '__main__':
    main()
